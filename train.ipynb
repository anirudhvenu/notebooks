{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch.utils.data as data\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import h5py\n",
    "import json\n",
    "import time\n",
    "import pdb\n",
    "import random\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = '~/notebooks'\n",
    "os.system('wget https://filebox.ece.vt.edu/~jiasenlu/codeRelease/visDial.pytorch/model/HCIAE-D-MLE.pth -P %s' %(path))\n",
    "os.system('wget https://filebox.ece.vt.edu/~jiasenlu/codeRelease/visDial.pytorch/model/HCIAE-G-MLE.pth -P %s' %(path))\n",
    "# os.system('wget https://filebox.ece.vt.edu/~jiasenlu/codeRelease/visDial.pytorch/model/HCIAE-G-DIS.pth -P %s' %(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('wget https://filebox.ece.vt.edu/~jiasenlu/codeRelease/visDial.pytorch/data/vdl_img_vgg.h5 -P %s' %(path))\n",
    "os.system('wget https://filebox.ece.vt.edu/~jiasenlu/codeRelease/visDial.pytorch/data/visdial_data.h5 -P %s' %(path))\n",
    "os.system('wget https://filebox.ece.vt.edu/~jiasenlu/codeRelease/visDial.pytorch/data/visdial_params.json -P %s' %(path))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('wget https://filebox.ece.vt.edu/~jiasenlu/codeRelease/visDial.pytorch/data/vdl_img_vgg_demo.h5 -P %s' %(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.system('wget https://filebox.ece.vt.edu/~jiasenlu/codeRelease/visDial.pytorch/data/visdial_data_demo.h5 -P %s' %(path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Random Seed: ', 5424)\n"
     ]
    }
   ],
   "source": [
    "manualSeed = random.randint(1, 10000) # fix seed\n",
    "print(\"Random Seed: \", manualSeed)\n",
    "random.seed(manualSeed)\n",
    "torch.manual_seed(manualSeed)\n",
    "np.random.seed(manualSeed)\n",
    "torch.cuda.manual_seed(manualSeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class train(data.Dataset): # torch wrapper\n",
    "    def __init__(self, input_img_h5, input_ques_h5, input_json, negative_sample, num_val, data_split):\n",
    "\n",
    "        print('DataLoader loading: %s' %data_split)\n",
    "        print('Loading image feature from %s' %input_img_h5)\n",
    "\n",
    "        if data_split == 'test':\n",
    "            split = 'val'\n",
    "        else:\n",
    "            split = 'train' # train and val split both corresponding to 'train'\n",
    "\n",
    "        f = json.load(open(input_json, 'r'))\n",
    "        self.itow = f['itow']\n",
    "        self.img_info = f['img_'+split]\n",
    "\n",
    "        # get the data split.\n",
    "        total_num = len(self.img_info)\n",
    "        if data_split == 'train':\n",
    "            s = 0\n",
    "            e = total_num - num_val\n",
    "        elif data_split == 'val':\n",
    "            s = total_num - num_val\n",
    "            e = total_num\n",
    "        else:\n",
    "            s = 0\n",
    "            e = total_num\n",
    "            \n",
    "        self.img_info = self.img_info[s:e]\n",
    "\n",
    "        print('%s number of data: %d' %(data_split, e-s))\n",
    "        # load the data.\n",
    "        f = h5py.File(input_img_h5, 'r')\n",
    "        self.imgs = f['images_'+split][s:e]\n",
    "        f.close()\n",
    "\n",
    "        print('Loading txt from %s' %input_ques_h5)\n",
    "        f = h5py.File(input_ques_h5, 'r')\n",
    "        self.ques = f['ques_'+split][s:e]\n",
    "        self.ans = f['ans_'+split][s:e]\n",
    "        self.cap = f['cap_'+split][s:e]\n",
    "\n",
    "        self.ques_len = f['ques_len_'+split][s:e]\n",
    "        self.ans_len = f['ans_len_'+split][s:e]\n",
    "        self.cap_len = f['cap_len_'+split][s:e]\n",
    "\n",
    "        self.ans_ids = f['ans_index_'+split][s:e]\n",
    "        self.opt_ids = f['opt_'+split][s:e]\n",
    "        self.opt_list = f['opt_list_'+split][:]\n",
    "        self.opt_len = f['opt_len_'+split][:]\n",
    "        f.close()\n",
    "\n",
    "        self.ques_length = self.ques.shape[2]\n",
    "        self.ans_length = self.ans.shape[2]\n",
    "        self.his_length = self.ques_length + self.ans_length\n",
    "        self.vocab_size = len(self.itow)+1\n",
    "\n",
    "        print('Vocab Size: %d' % self.vocab_size)\n",
    "        self.split = split\n",
    "        self.rnd = 10\n",
    "        self.negative_sample = negative_sample\n",
    "\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        # get the image\n",
    "        img = torch.from_numpy(self.imgs[index])\n",
    "\n",
    "        # get the history\n",
    "        his = np.zeros((self.rnd, self.his_length))\n",
    "        his[0,self.his_length-self.cap_len[index]:] = self.cap[index,:self.cap_len[index]]\n",
    "\n",
    "        ques = np.zeros((self.rnd, self.ques_length))\n",
    "        ans = np.zeros((self.rnd, self.ans_length+1))\n",
    "        ans_target = np.zeros((self.rnd, self.ans_length+1))\n",
    "        ques_ori = np.zeros((self.rnd, self.ques_length))\n",
    "\n",
    "        opt_ans = np.zeros((self.rnd, self.negative_sample, self.ans_length+1))\n",
    "        ans_len = np.zeros((self.rnd))\n",
    "        opt_ans_len = np.zeros((self.rnd, self.negative_sample))\n",
    "\n",
    "        ans_idx = np.zeros((self.rnd))\n",
    "        opt_ans_idx = np.zeros((self.rnd, self.negative_sample))\n",
    "\n",
    "        for i in range(self.rnd):\n",
    "            # get the index\n",
    "            q_len = self.ques_len[index, i]\n",
    "            a_len = self.ans_len[index, i]\n",
    "            qa_len = q_len + a_len\n",
    "\n",
    "            if i+1 < self.rnd:\n",
    "                his[i+1, self.his_length-qa_len:self.his_length-a_len] = self.ques[index, i, :q_len]\n",
    "                his[i+1, self.his_length-a_len:] = self.ans[index, i, :a_len]\n",
    "\n",
    "            ques[i, self.ques_length-q_len:] = self.ques[index, i, :q_len]\n",
    "\n",
    "            ques_ori[i, :q_len] = self.ques[index, i, :q_len]\n",
    "            ans[i, 1:a_len+1] = self.ans[index, i, :a_len]\n",
    "            ans[i, 0] = self.vocab_size\n",
    "\n",
    "            ans_target[i, :a_len] = self.ans[index, i, :a_len]\n",
    "            ans_target[i, a_len] = self.vocab_size\n",
    "            ans_len[i] = self.ans_len[index, i]\n",
    "\n",
    "            opt_ids = self.opt_ids[index, i] # since python start from 0\n",
    "            # random select the negative samples.\n",
    "            ans_idx[i] = opt_ids[self.ans_ids[index, i]]\n",
    "            # exclude the gt index.\n",
    "            opt_ids = np.delete(opt_ids, ans_idx[i], 0)\n",
    "            random.shuffle(opt_ids)\n",
    "            for j in range(self.negative_sample):\n",
    "                ids = opt_ids[j]\n",
    "                opt_ans_idx[i,j] = ids\n",
    "\n",
    "                opt_len = self.opt_len[ids]\n",
    "\n",
    "                opt_ans_len[i, j] = opt_len\n",
    "                opt_ans[i, j, :opt_len] = self.opt_list[ids,:opt_len]\n",
    "                opt_ans[i, j, opt_len] = self.vocab_size\n",
    "\n",
    "        his = torch.from_numpy(his)\n",
    "        ques = torch.from_numpy(ques)\n",
    "        ans = torch.from_numpy(ans)\n",
    "        ans_target = torch.from_numpy(ans_target)\n",
    "        ques_ori = torch.from_numpy(ques_ori)\n",
    "        ans_len = torch.from_numpy(ans_len)\n",
    "        opt_ans_len = torch.from_numpy(opt_ans_len)\n",
    "        opt_ans = torch.from_numpy(opt_ans)\n",
    "        ans_idx = torch.from_numpy(ans_idx)\n",
    "        opt_ans_idx = torch.from_numpy(opt_ans_idx)\n",
    "        return img, his, ques, ans, ans_target, ans_len, ans_idx, ques_ori, \\\n",
    "                opt_ans, opt_ans_len, opt_ans_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ques.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class validate(data.Dataset): # torch wrapper\n",
    "    def __init__(self, input_img_h5, input_ques_h5, input_json, negative_sample, num_val, data_split):\n",
    "\n",
    "        print('DataLoader loading: %s' %data_split)\n",
    "        print('Loading image feature from %s' %input_img_h5)\n",
    "\n",
    "        if data_split == 'test':\n",
    "            split = 'val'\n",
    "        else:\n",
    "            split = 'train' # train and val split both corresponding to 'train'\n",
    "\n",
    "        f = json.load(open(input_json, 'r'))\n",
    "        self.itow = f['itow']\n",
    "        self.img_info = f['img_'+split]\n",
    "\n",
    "        # get the data split.\n",
    "        total_num = len(self.img_info)\n",
    "        if data_split == 'train':\n",
    "            s = 0\n",
    "            e = total_num - num_val\n",
    "        elif data_split == 'val':\n",
    "            s = total_num - num_val\n",
    "            e = total_num\n",
    "        else:\n",
    "            s = 0\n",
    "            e = total_num\n",
    "\n",
    "        self.img_info = self.img_info[s:e]\n",
    "        print('%s number of data: %d' %(data_split, e-s))\n",
    "\n",
    "        # load the data.\n",
    "        f = h5py.File(input_img_h5, 'r')\n",
    "###########################################################################################\n",
    "#CHANGE THIS HERE FOR NON DEMO TRAINING SET\n",
    "###########################################################################################\n",
    "        split = 'train'\n",
    "        self.imgs = f['images_'+split][s:e]\n",
    "\n",
    "        f.close()\n",
    "\n",
    "        print('Loading txt from %s' %input_ques_h5)\n",
    "        f = h5py.File(input_ques_h5, 'r')\n",
    "        self.ques = f['ques_'+split][s:e]\n",
    "        self.ans = f['ans_'+split][s:e]\n",
    "        self.cap = f['cap_'+split][s:e]\n",
    "\n",
    "        self.ques_len = f['ques_len_'+split][s:e]\n",
    "        self.ans_len = f['ans_len_'+split][s:e]\n",
    "        self.cap_len = f['cap_len_'+split][s:e]\n",
    "\n",
    "        self.ans_ids = f['ans_index_'+split][s:e]\n",
    "        self.opt_ids = f['opt_'+split][s:e]\n",
    "        self.opt_list = f['opt_list_'+split][:]\n",
    "        self.opt_len = f['opt_len_'+split][:]\n",
    "        f.close()\n",
    "\n",
    "        self.ques_length = self.ques.shape[2]\n",
    "        self.ans_length = self.ans.shape[2]\n",
    "        self.his_length = self.ques_length + self.ans_length\n",
    "        self.vocab_size = len(self.itow)+1\n",
    "\n",
    "        print('Vocab Size: %d' % self.vocab_size)\n",
    "        self.split = split\n",
    "        self.rnd = 10\n",
    "        self.negative_sample = negative_sample\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        # get the image\n",
    "        img_id = self.img_info[index]['imgId']\n",
    "        img = torch.from_numpy(self.imgs[index])\n",
    "        # get the history\n",
    "        his = np.zeros((self.rnd, self.his_length))\n",
    "        his[0,self.his_length-self.cap_len[index]:] = self.cap[index,:self.cap_len[index]]\n",
    "\n",
    "        ques = np.zeros((self.rnd, self.ques_length))\n",
    "        ans = np.zeros((self.rnd, self.ans_length+1))\n",
    "        ans_target = np.zeros((self.rnd, self.ans_length+1))\n",
    "        quesL = np.zeros((self.rnd, self.ques_length))\n",
    "\n",
    "        opt_ans = np.zeros((self.rnd, 100, self.ans_length+1))\n",
    "        ans_ids = np.zeros(self.rnd)\n",
    "        opt_ans_target = np.zeros((self.rnd, 100, self.ans_length+1))\n",
    "\n",
    "        ans_len = np.zeros((self.rnd))\n",
    "        opt_ans_len = np.zeros((self.rnd, 100))\n",
    "\n",
    "\n",
    "        for i in range(self.rnd):\n",
    "            # get the index\n",
    "            q_len = self.ques_len[index, i]\n",
    "            a_len = self.ans_len[index, i]\n",
    "            qa_len = q_len + a_len\n",
    "\n",
    "            if i+1 < self.rnd:\n",
    "                ques_ans = np.concatenate([self.ques[index, i, :q_len], self.ans[index, i, :a_len]])\n",
    "                his[i+1, self.his_length-qa_len:] = ques_ans\n",
    "\n",
    "            ques[i, self.ques_length-q_len:] = self.ques[index, i, :q_len]\n",
    "            quesL[i, :q_len] = self.ques[index, i, :q_len]\n",
    "            ans[i, 1:a_len+1] = self.ans[index, i, :a_len]\n",
    "            ans[i, 0] = self.vocab_size\n",
    "\n",
    "            ans_target[i, :a_len] = self.ans[index, i, :a_len]\n",
    "            ans_target[i, a_len] = self.vocab_size\n",
    "\n",
    "            ans_ids[i] = self.ans_ids[index, i] # since python start from 0\n",
    "            opt_ids = self.opt_ids[index, i] # since python start from 0\n",
    "            ans_len[i] = self.ans_len[index, i]\n",
    "            ans_idx = self.ans_ids[index, i]\n",
    "\n",
    "            for j, ids in enumerate(opt_ids):\n",
    "                opt_len = self.opt_len[ids]\n",
    "                opt_ans[i, j, 1:opt_len+1] = self.opt_list[ids,:opt_len]\n",
    "                opt_ans[i, j, 0] = self.vocab_size\n",
    "\n",
    "                opt_ans_target[i, j,:opt_len] = self.opt_list[ids,:opt_len]\n",
    "                opt_ans_target[i, j,opt_len] = self.vocab_size\n",
    "                opt_ans_len[i, j] = opt_len\n",
    "\n",
    "        opt_ans = torch.from_numpy(opt_ans)\n",
    "        opt_ans_target = torch.from_numpy(opt_ans_target)\n",
    "        ans_ids = torch.from_numpy(ans_ids)\n",
    "\n",
    "        his = torch.from_numpy(his)\n",
    "        ques = torch.from_numpy(ques)\n",
    "        ans = torch.from_numpy(ans)\n",
    "        ans_target = torch.from_numpy(ans_target)\n",
    "        quesL = torch.from_numpy(quesL)\n",
    "\n",
    "        ans_len = torch.from_numpy(ans_len)\n",
    "        opt_ans_len = torch.from_numpy(opt_ans_len)\n",
    "\n",
    "        return img, his, ques, ans, ans_target, quesL, opt_ans, \\\n",
    "                    opt_ans_target, ans_ids, ans_len, opt_ans_len, img_id\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.ques.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader loading: train\n",
      "Loading image feature from vdl_img_vgg_demo.h5\n",
      "train number of data: 81783\n",
      "Loading txt from visdial_data_demo.h5\n",
      "Vocab Size: 8964\n"
     ]
    }
   ],
   "source": [
    "input_img_h5 = 'vdl_img_vgg_demo.h5'\n",
    "input_ques_h5 = 'visdial_data_demo.h5'\n",
    "input_json = 'visdial_params.json'\n",
    "negative_sample = 20\n",
    "num_val = 1000\n",
    "dataset = train(input_img_h5=input_img_h5, input_ques_h5=input_ques_h5,\n",
    "                input_json=input_json, negative_sample = negative_sample,\n",
    "                num_val = num_val, data_split = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoader loading: test\n",
      "Loading image feature from vdl_img_vgg_demo.h5\n",
      "test number of data: 40504\n",
      "Loading txt from visdial_data_demo.h5\n",
      "Vocab Size: 8964\n"
     ]
    }
   ],
   "source": [
    "dataset_val = validate(input_img_h5=input_img_h5, input_ques_h5=input_ques_h5,\n",
    "                input_json=input_json, negative_sample = negative_sample,\n",
    "                num_val = num_val, data_split = 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'images_train']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = h5py.File(input_img_h5, 'r')\n",
    "f.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'ans_index_train',\n",
       " u'ans_len_train',\n",
       " u'ans_train',\n",
       " u'cap_len_train',\n",
       " u'cap_train',\n",
       " u'opt_len_train',\n",
       " u'opt_list_train',\n",
       " u'opt_train',\n",
       " u'ques_len_train',\n",
       " u'ques_train']"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = h5py.File(input_ques_h5, 'r')\n",
    "f.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batchSize = 100\n",
    "num_workers = 0\n",
    "dloader = torch.utils.data.DataLoader(dataset, batch_size=batchSize,\n",
    "                                         shuffle=True, num_workers=int(num_workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataloader_val = torch.utils.data.DataLoader(dataset_val, batch_size=1,\n",
    "                                         shuffle=False, num_workers=int(num_workers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data_iter1 = iter(dloader)\n",
    "data = data_iter1.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image, history, question, answer, answerT, answerLen, answerIdx, questionL, \\\n",
    "        opt_answerT, opt_answerLen, opt_answerIdx = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _netE(nn.Module):\n",
    "    \"\"\"Container module with an encoder, a recurrent module, and a decoder.\"\"\"\n",
    "\n",
    "    def __init__(self, rnn_type, ninp, nhid, nlayers, dropout, img_feat_size):\n",
    "        super(_netE, self).__init__()\n",
    "\n",
    "        self.d = dropout\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.nhid = nhid\n",
    "        self.ninp = ninp\n",
    "        self.img_embed = nn.Linear(img_feat_size, nhid)\n",
    "\n",
    "        self.ques_rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
    "        self.his_rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
    "\n",
    "        self.Wq_1 = nn.Linear(self.nhid, self.nhid)\n",
    "        self.Wh_1 = nn.Linear(self.nhid, self.nhid)\n",
    "        self.Wa_1 = nn.Linear(self.nhid, 1)\n",
    "\n",
    "        self.Wq_2 = nn.Linear(self.nhid, self.nhid)\n",
    "        self.Wh_2 = nn.Linear(self.nhid, self.nhid)\n",
    "        self.Wi_2 = nn.Linear(self.nhid, self.nhid)\n",
    "        self.Wa_2 = nn.Linear(self.nhid, 1)\n",
    "\n",
    "        self.fc1 = nn.Linear(self.nhid*3, self.ninp)\n",
    "\n",
    "    def forward(self, ques_emb, his_emb, img_raw, ques_hidden, his_hidden, rnd):\n",
    "\n",
    "        img_emb = F.tanh(self.img_embed(img_raw))\n",
    "\n",
    "        ques_feat, ques_hidden = self.ques_rnn(ques_emb, ques_hidden)\n",
    "        ques_feat = ques_feat[-1]\n",
    "\n",
    "        his_feat, his_hidden = self.his_rnn(his_emb, his_hidden)\n",
    "        his_feat = his_feat[-1]\n",
    "\n",
    "        ques_emb_1 = self.Wq_1(ques_feat).view(-1, 1, self.nhid)\n",
    "        his_emb_1 = self.Wh_1(his_feat).view(-1, rnd, self.nhid)\n",
    "\n",
    "        atten_emb_1 = F.tanh(his_emb_1 + ques_emb_1.expand_as(his_emb_1))\n",
    "        his_atten_weight = F.softmax(self.Wa_1(F.dropout(atten_emb_1, self.d, training=self.training\n",
    "                                                ).view(-1, self.nhid)).view(-1, rnd))\n",
    "\n",
    "        his_attn_feat = torch.bmm(his_atten_weight.view(-1, 1, rnd),\n",
    "                                        his_feat.view(-1, rnd, self.nhid))\n",
    "\n",
    "        his_attn_feat = his_attn_feat.view(-1, self.nhid)\n",
    "        ques_emb_2 = self.Wq_2(ques_feat).view(-1, 1, self.nhid)\n",
    "        his_emb_2 = self.Wh_2(his_attn_feat).view(-1, 1, self.nhid)\n",
    "        img_emb_2 = self.Wi_2(img_emb).view(-1, 49, self.nhid)\n",
    "\n",
    "        atten_emb_2 = F.tanh(img_emb_2 + ques_emb_2.expand_as(img_emb_2) + \\\n",
    "                                    his_emb_2.expand_as(img_emb_2))\n",
    "\n",
    "        img_atten_weight = F.softmax(self.Wa_2(F.dropout(atten_emb_2, self.d, training=self.training\n",
    "                                                ).view(-1, self.nhid)).view(-1, 49))\n",
    "\n",
    "        img_attn_feat = torch.bmm(img_atten_weight.view(-1, 1, 49),\n",
    "                                        img_emb.view(-1, 49, self.nhid))\n",
    "\n",
    "        concat_feat = torch.cat((ques_feat, his_attn_feat.view(-1, self.nhid), \\\n",
    "                                 img_attn_feat.view(-1, self.nhid)),1)\n",
    "\n",
    "        encoder_feat = F.tanh(self.fc1(F.dropout(concat_feat, self.d, training=self.training)))\n",
    "\n",
    "        return encoder_feat, ques_hidden\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (Variable(weight.new(self.nlayers, bsz, self.nhid).zero_()),\n",
    "                    Variable(weight.new(self.nlayers, bsz, self.nhid).zero_()))\n",
    "        else:\n",
    "            return Variable(weight.new(self.nlayers, bsz, self.nhid).zero_())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _netW(nn.Module):\n",
    "    def __init__(self, ntoken, ninp, dropout):\n",
    "        super(_netW, self).__init__()\n",
    "        self.word_embed = nn.Embedding(ntoken+1, ninp).cuda()\n",
    "        self.Linear = share_Linear(self.word_embed.weight).cuda()\n",
    "#         self.word_embed = nn.Embedding(ntoken+1, ninp)\n",
    "#         self.Linear = share_Linear(self.word_embed.weight)\n",
    "        self.init_weights()\n",
    "        self.d = dropout\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.1\n",
    "        self.word_embed.weight.data.uniform_(-initrange, initrange)\n",
    "\n",
    "    def forward(self, input, format ='index'):\n",
    "        if format == 'onehot':\n",
    "            out = F.dropout(self.Linear(input), self.d, training=self.training)\n",
    "        elif format == 'index':\n",
    "            out = F.dropout(self.word_embed(input), self.d, training=self.training)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (Variable(weight.new(self.nlayers, bsz, self.nhid).zero_()),\n",
    "                    Variable(weight.new(self.nlayers, bsz, self.nhid).zero_()))\n",
    "        else:\n",
    "            return Variable(weight.new(self.nlayers, bsz, self.nhid).zero_())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class _netD(nn.Module):\n",
    "    \"\"\"\n",
    "    Given the real/wrong/fake answer, use a RNN (LSTM) to embed the answer.\n",
    "    \"\"\"\n",
    "    def __init__(self, rnn_type, ninp, nhid, nlayers, ntoken, dropout):\n",
    "        super(_netD, self).__init__()\n",
    "\n",
    "        self.rnn_type = rnn_type\n",
    "        self.nhid = nhid\n",
    "        self.nlayers = nlayers\n",
    "        self.ntoken = ntoken\n",
    "        self.ninp = ninp\n",
    "        self.d = dropout\n",
    "\n",
    "        self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers)\n",
    "        self.W1 = nn.Linear(self.nhid, self.nhid)\n",
    "        self.W2 = nn.Linear(self.nhid, 1)\n",
    "        self.fc = nn.Linear(nhid, ninp)\n",
    "\n",
    "    def forward(self, input_feat, idx, hidden, vocab_size):\n",
    "\n",
    "        output, _ = self.rnn(input_feat, hidden)\n",
    "        mask = idx.data.eq(0)  # generate the mask\n",
    "        mask[idx.data == vocab_size] = 1 # also set the last token to be 1\n",
    "        if isinstance(input_feat, Variable):\n",
    "            mask = Variable(mask, volatile=input_feat.volatile)\n",
    "\n",
    "        # Doing self attention here.\n",
    "        atten = self.W2(F.dropout(F.tanh(self.W1(output.view(-1, self.nhid))), self.d, training=self.training)).view(idx.size())\n",
    "        atten.masked_fill_(mask, -99999)\n",
    "        weight = F.softmax(atten.t()).view(-1,1,idx.size(0))\n",
    "        feat = torch.bmm(weight, output.transpose(0,1)).view(-1,self.nhid)\n",
    "        feat = F.dropout(feat, self.d, training=self.training)\n",
    "        transform_output = F.tanh(self.fc(feat))\n",
    "\n",
    "        return transform_output\n",
    "\n",
    "    def init_hidden(self, bsz):\n",
    "        weight = next(self.parameters()).data\n",
    "        if self.rnn_type == 'LSTM':\n",
    "            return (Variable(weight.new(self.nlayers, bsz, self.nhid).zero_()),\n",
    "                    Variable(weight.new(self.nlayers, bsz, self.nhid).zero_()))\n",
    "        else:\n",
    "            return Variable(weight.new(self.nlayers, bsz, self.nhid).zero_())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class nPairLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Given the right, fake, wrong, wrong_sampled embedding, use the N Pair Loss\n",
    "    objective (which is an extension to the triplet loss)\n",
    "\n",
    "    Loss = log(1+exp(feat*wrong - feat*right + feat*fake - feat*right)) + L2 norm.\n",
    "\n",
    "    Improved Deep Metric Learning with Multi-class N-pair Loss Objective (NIPS)\n",
    "    \"\"\"\n",
    "    def __init__(self, ninp, margin):\n",
    "        super(nPairLoss, self).__init__()\n",
    "        self.ninp = ninp\n",
    "        self.margin = np.log(margin)\n",
    "\n",
    "    def forward(self, feat, right, wrong, batch_wrong, fake=None, fake_diff_mask=None):\n",
    "\n",
    "        num_wrong = wrong.size(1)\n",
    "        batch_size = feat.size(0)\n",
    "\n",
    "        feat = feat.view(-1, self.ninp, 1)\n",
    "        right_dis = torch.bmm(right.view(-1, 1, self.ninp), feat)\n",
    "        wrong_dis = torch.bmm(wrong, feat)\n",
    "        batch_wrong_dis = torch.bmm(batch_wrong, feat)\n",
    "\n",
    "        wrong_score = torch.sum(torch.exp(wrong_dis - right_dis.expand_as(wrong_dis)),1) \\\n",
    "                + torch.sum(torch.exp(batch_wrong_dis - right_dis.expand_as(batch_wrong_dis)),1)\n",
    "\n",
    "        loss_dis = torch.sum(torch.log(wrong_score + 1))\n",
    "        loss_norm = right.norm() + feat.norm() + wrong.norm() + batch_wrong.norm()\n",
    "\n",
    "        if fake:\n",
    "            fake_dis = torch.bmm(fake.view(-1, 1, self.ninp), feat)\n",
    "            fake_score = torch.masked_select(torch.exp(fake_dis - right_dis), fake_diff_mask)\n",
    "\n",
    "            margin_score = F.relu(torch.log(fake_score + 1) - self.margin)\n",
    "            loss_fake = torch.sum(margin_score)\n",
    "            loss_dis += loss_fake\n",
    "            loss_norm += fake.norm()\n",
    "\n",
    "        loss = (loss_dis + 0.1 * loss_norm) / batch_size\n",
    "        if fake:\n",
    "            return loss, loss_fake.data[0] / batch_size\n",
    "        else:\n",
    "            return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class share_Linear(Module):\n",
    "    r\"\"\"Applies a linear transformation to the incoming data: :math:`y = Ax + b`\n",
    "    Args:\n",
    "        in_features: size of each input sample\n",
    "        out_features: size of each output sample\n",
    "        bias: If set to False, the layer will not learn an additive bias. Default: True\n",
    "    Shape:\n",
    "        - Input: :math:`(N, in\\_features)`\n",
    "        - Output: :math:`(N, out\\_features)`\n",
    "    Attributes:\n",
    "        weight: the learnable weights of the module of shape (out_features x in_features)\n",
    "        bias:   the learnable bias of the module of shape (out_features)\n",
    "    Examples::\n",
    "        >>> m = nn.Linear(20, 30)\n",
    "        >>> input = autograd.Variable(torch.randn(128, 20))\n",
    "        >>> output = m(input)\n",
    "        >>> print(output.size())\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weight):\n",
    "        super(share_Linear, self).__init__()\n",
    "        self.in_features = weight.size(0)\n",
    "        self.out_features = weight.size(1)\n",
    "        self.weight = weight.t()\n",
    "        self.register_parameter('bias', None)\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.linear(input, self.weight, self.bias)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "            + str(self.in_features) + ' -> ' \\\n",
    "            + str(self.out_features) + ')'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_neg = negative_sample\n",
    "vocab_size = dataset.vocab_size\n",
    "ques_length = dataset.ques_length\n",
    "ans_length = dataset.ans_length + 1\n",
    "his_length = dataset.ans_length + dataset.ques_length\n",
    "itow = dataset.itow\n",
    "img_feat_size = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(_netD(\n",
       "   (rnn): LSTM(300, 512)\n",
       "   (W1): Linear(in_features=512, out_features=512, bias=True)\n",
       "   (W2): Linear(in_features=512, out_features=1, bias=True)\n",
       "   (fc): Linear(in_features=512, out_features=300, bias=True)\n",
       " ), nPairLoss(\n",
       " ))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = 'LSTM'\n",
    "ninp = 300\n",
    "nhid = 512\n",
    "nlayers = 1 \n",
    "dropout = 0.5\n",
    "margin = 2 \n",
    "\n",
    "netE = _netE(model, ninp, nhid, nlayers, dropout, img_feat_size)\n",
    "netW = _netW(vocab_size, ninp, dropout)\n",
    "netD = _netD(model, ninp, nhid, nlayers, vocab_size, dropout)\n",
    "critD =nPairLoss(ninp, margin)\n",
    "\n",
    "netW.cuda(), netE.cuda(),\n",
    "netD.cuda(), critD.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_batch_neg(answerIdx, negAnswerIdx, sample_idx, num_sample):\n",
    "    \"\"\"\n",
    "    input:\n",
    "    answerIdx: batch_size\n",
    "    negAnswerIdx: batch_size x opt.negative_sample\n",
    "\n",
    "    output:\n",
    "    sample_idx = batch_size x num_sample\n",
    "    \"\"\"\n",
    "\n",
    "    batch_size = answerIdx.size(0)\n",
    "    num_neg = negAnswerIdx.size(0) * negAnswerIdx.size(1)\n",
    "    negAnswerIdx = negAnswerIdx.clone().view(-1)\n",
    "    for b in range(batch_size):\n",
    "        gt_idx = answerIdx[b]\n",
    "        for n in range(num_sample):\n",
    "            while True:\n",
    "                rand = int(random.random() * num_neg)\n",
    "                neg_idx = negAnswerIdx[rand]\n",
    "                if gt_idx != neg_idx:\n",
    "                    sample_idx.data[b, n] = rand\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def repackage_hidden(h, batch_size):\n",
    "    \"\"\"Wraps hidden states in new Variables, to detach them from their history.\"\"\"\n",
    "    if type(h) == Variable:\n",
    "        return Variable(h.data.resize_(h.size(0), batch_size, h.size(2)).zero_())\n",
    "    else:\n",
    "        return tuple(repackage_hidden(v, batch_size) for v in h)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    netW.train()\n",
    "    netE.train()\n",
    "    netD.train()\n",
    "\n",
    "#     lr = adjust_learning_rate(optimizer, epoch, opt.lr)\n",
    "\n",
    "    ques_hidden = netE.init_hidden(batchSize)\n",
    "    hist_hidden = netE.init_hidden(batchSize)\n",
    "\n",
    "    real_hidden = netD.init_hidden(batchSize)\n",
    "    wrong_hidden = netD.init_hidden(batchSize)\n",
    "\n",
    "    data_iter = iter(dloader)\n",
    "\n",
    "    average_loss = 0\n",
    "    count = 0\n",
    "    i = 0\n",
    "\n",
    "    while i < len(dloader):\n",
    "\n",
    "        t1 = time.time()\n",
    "        data = data_iter.next()\n",
    "        image, history, question, answer, answerT, answerLen, answerIdx, questionL, \\\n",
    "                                    opt_answerT, opt_answerLen, opt_answerIdx = data\n",
    "\n",
    "        batch_size = question.size(0)\n",
    "        image = image.view(-1, img_feat_size)\n",
    "        img_input.data.resize_(image.size()).copy_(image)\n",
    "\n",
    "        for rnd in range(10):\n",
    "            netW.zero_grad()\n",
    "            netE.zero_grad()\n",
    "            netD.zero_grad()\n",
    "            # get the corresponding round QA and history.\n",
    "            ques = question[:,rnd,:].t()\n",
    "            his = history[:,:rnd+1,:].clone().view(-1, his_length).t()\n",
    "\n",
    "            ans = answer[:,rnd,:].t()\n",
    "            tans = answerT[:,rnd,:].t()\n",
    "            wrong_ans = opt_answerT[:,rnd,:].clone().view(-1, ans_length).t()\n",
    "\n",
    "            real_len = answerLen[:,rnd]\n",
    "            wrong_len = opt_answerLen[:,rnd,:].clone().view(-1)\n",
    "\n",
    "            ques_input.data.resize_(ques.size()).copy_(ques)\n",
    "            his_input.data.resize_(his.size()).copy_(his)\n",
    "\n",
    "            ans_input.data.resize_(ans.size()).copy_(ans)\n",
    "            ans_target.data.resize_(tans.size()).copy_(tans)\n",
    "            wrong_ans_input.data.resize_(wrong_ans.size()).copy_(wrong_ans)\n",
    "\n",
    "            # sample in-batch negative index\n",
    "            batch_sample_idx.data.resize_(batch_size, neg_batch_sample).zero_()\n",
    "            sample_batch_neg(answerIdx[:,rnd], opt_answerIdx[:,rnd,:], batch_sample_idx, neg_batch_sample)\n",
    "\n",
    "            ques_emb = netW(ques_input, format = 'index')\n",
    "            his_emb = netW(his_input, format = 'index')\n",
    "\n",
    "            ques_hidden = repackage_hidden(ques_hidden, batch_size)\n",
    "            hist_hidden = repackage_hidden(hist_hidden, his_input.size(1))\n",
    "\n",
    "            featD, ques_hidden = netE(ques_emb, his_emb, img_input, \\\n",
    "                                                ques_hidden, hist_hidden, rnd+1)\n",
    "\n",
    "            ans_real_emb = netW(ans_target, format='index')\n",
    "            ans_wrong_emb = netW(wrong_ans_input, format='index')\n",
    "\n",
    "            real_hidden = repackage_hidden(real_hidden, batch_size)\n",
    "            wrong_hidden = repackage_hidden(wrong_hidden, ans_wrong_emb.size(1))\n",
    "\n",
    "            real_feat = netD(ans_real_emb, ans_target, real_hidden, vocab_size)\n",
    "            wrong_feat = netD(ans_wrong_emb, wrong_ans_input, wrong_hidden, vocab_size)\n",
    "\n",
    "            batch_wrong_feat = wrong_feat.index_select(0, batch_sample_idx.view(-1))\n",
    "            wrong_feat = wrong_feat.view(batch_size, -1, ninp)\n",
    "            batch_wrong_feat = batch_wrong_feat.view(batch_size, -1, ninp)\n",
    "\n",
    "            nPairLoss = critD(featD, real_feat, wrong_feat, batch_wrong_feat)\n",
    "\n",
    "            average_loss += nPairLoss.data[0]\n",
    "            nPairLoss.backward()\n",
    "            optimizer.step()\n",
    "            count += 1\n",
    "\n",
    "        i += 1\n",
    "        if i % log_interval == 0:\n",
    "            average_loss /= count\n",
    "            print(\"step {} / {} (epoch {}), g_loss {:.3f}, lr = {:.6f}\"\\\n",
    "                .format(i, len(dataloader), epoch, average_loss, lr))\n",
    "            average_loss = 0\n",
    "            count = 0\n",
    "\n",
    "    return average_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def val():\n",
    "    netE.eval()\n",
    "    netW.eval()\n",
    "    netD.eval()\n",
    "\n",
    "    n_neg = 100\n",
    "    data_iter_val = iter(dataloader_val)\n",
    "    ques_hidden = netE.init_hidden(batchSize)\n",
    "    hist_hidden = netE.init_hidden(batchSize)\n",
    "\n",
    "    opt_hidden = netD.init_hidden(batchSize)\n",
    "    i = 0\n",
    "\n",
    "    average_loss = 0\n",
    "    rank_all_tmp = []\n",
    "\n",
    "    while i < len(dataloader_val):\n",
    "        data = data_iter_val.next()\n",
    "        image, history, question, answer, answerT, questionL, opt_answer, \\\n",
    "                opt_answerT, answer_ids, answerLen, opt_answerLen, img_id  = data\n",
    "\n",
    "        batch_size = question.size(0)\n",
    "        image = image.view(-1, img_feat_size)\n",
    "        #image = l2_norm(image)\n",
    "        img_input.data.resize_(image.size()).copy_(image)\n",
    "\n",
    "        for rnd in range(10):\n",
    "            # get the corresponding round QA and history.\n",
    "            ques = question[:,rnd,:].t()\n",
    "            his = history[:,:rnd+1,:].clone().view(-1, his_length).t()\n",
    "\n",
    "            opt_ans = opt_answerT[:,rnd,:].clone().view(-1, ans_length).t()\n",
    "            gt_id = answer_ids[:,rnd]\n",
    "\n",
    "            ques_input.data.resize_(ques.size()).copy_(ques)\n",
    "            his_input.data.resize_(his.size()).copy_(his)\n",
    "\n",
    "            opt_ans_input.data.resize_(opt_ans.size()).copy_(opt_ans)\n",
    "            gt_index.data.resize_(gt_id.size()).copy_(gt_id)\n",
    "            opt_len = opt_answerLen[:,rnd,:].clone().view(-1)\n",
    "\n",
    "            ques_emb = netW(ques_input, format = 'index')\n",
    "            his_emb = netW(his_input, format = 'index')\n",
    "\n",
    "            ques_hidden = repackage_hidden(ques_hidden, batch_size)\n",
    "            hist_hidden = repackage_hidden(hist_hidden, his_input.size(1))\n",
    "\n",
    "            featD, ques_hidden = netE(ques_emb, his_emb, img_input, \\\n",
    "                                                ques_hidden, hist_hidden, rnd+1)\n",
    "\n",
    "            opt_ans_emb = netW(opt_ans_input, format = 'index')\n",
    "            opt_hidden = repackage_hidden(opt_hidden, opt_ans_input.size(1))\n",
    "            opt_feat = netD(opt_ans_emb, opt_ans_input, opt_hidden, vocab_size)\n",
    "            opt_feat = opt_feat.view(batch_size, -1, ninp)\n",
    "\n",
    "            #ans_emb = ans_emb.view(ans_length, -1, 100, opt.nhid)\n",
    "            featD = featD.view(-1, ninp, 1)\n",
    "            score = torch.bmm(opt_feat, featD)\n",
    "            score = score.view(-1, 100)\n",
    "\n",
    "            for b in range(batch_size):\n",
    "                gt_index.data[b] = gt_index.data[b] + b*100\n",
    "\n",
    "            gt_score = score.view(-1).index_select(0, gt_index)\n",
    "            sort_score, sort_idx = torch.sort(score, 1, descending=True)\n",
    "\n",
    "            count = sort_score.gt(gt_score.view(-1,1).expand_as(sort_score))\n",
    "            rank = count.sum(1) + 1\n",
    "            rank_all_tmp += list(rank.view(-1).data.cpu().numpy())\n",
    "            \n",
    "        i += 1\n",
    "        sys.stdout.write('Evaluating: {:d}/{:d}  \\r' \\\n",
    "          .format(i, len(dataloader_val)))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    return rank_all_tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_input = torch.FloatTensor(batchSize)\n",
    "ques_input = torch.LongTensor(ques_length, batchSize)\n",
    "his_input = torch.LongTensor(his_length, batchSize)\n",
    "\n",
    "# answer input\n",
    "ans_input = torch.LongTensor(ans_length, batchSize)\n",
    "ans_target = torch.LongTensor(ans_length, batchSize)\n",
    "wrong_ans_input = torch.LongTensor(ans_length, batchSize)\n",
    "sample_ans_input = torch.LongTensor(1, batchSize)\n",
    "opt_ans_input = torch.LongTensor(ans_length, batchSize)\n",
    "\n",
    "batch_sample_idx = torch.LongTensor(batchSize)\n",
    "fake_diff_mask = torch.ByteTensor(batchSize)\n",
    "fake_len = torch.LongTensor(batchSize)\n",
    "noise_input = torch.FloatTensor(batchSize)\n",
    "gt_index = torch.LongTensor(batchSize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ques_input, his_input, img_input = ques_input.cuda(), his_input.cuda(), img_input.cuda()\n",
    "ans_input, ans_target = ans_input.cuda(), ans_target.cuda()\n",
    "wrong_ans_input = wrong_ans_input.cuda()\n",
    "sample_ans_input = sample_ans_input.cuda()\n",
    "\n",
    "fake_len = fake_len.cuda()\n",
    "noise_input = noise_input.cuda()\n",
    "batch_sample_idx = batch_sample_idx.cuda()\n",
    "fake_diff_mask = fake_diff_mask.cuda()\n",
    "opt_ans_input = opt_ans_input.cuda()\n",
    "gt_index = gt_index.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ques_input = Variable(ques_input)\n",
    "img_input = Variable(img_input)\n",
    "his_input = Variable(his_input)\n",
    "\n",
    "ans_input = Variable(ans_input)\n",
    "ans_target = Variable(ans_target)\n",
    "wrong_ans_input = Variable(wrong_ans_input)\n",
    "sample_ans_input = Variable(sample_ans_input)\n",
    "\n",
    "noise_input = Variable(noise_input)\n",
    "batch_sample_idx = Variable(batch_sample_idx)\n",
    "fake_diff_mask = Variable(fake_diff_mask)\n",
    "opt_ans_input = Variable(opt_ans_input)\n",
    "gt_index = Variable(gt_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:44: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:57: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/ipykernel/__main__.py:31: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 learningRate 0.000400 train loss 275.205922 Time: 31.011743\n",
      "Evaluating ... \n",
      "0/1000: mrr: 0.406669 R1: 0.289100 R5 0.505000 R10 0.596700 Mean 15.047500\n",
      "Epoch: 1 learningRate 0.000400 train loss 270.737673 Time: 31.040416\n",
      "Evaluating ... \n",
      "1/1000: mrr: 0.423115 R1: 0.305800 R5 0.523200 R10 0.618400 Mean 13.920100\n",
      "Epoch: 2 learningRate 0.000400 train loss 262.934155 Time: 31.017680\n",
      "Evaluating ... \n",
      "2/1000: mrr: 0.424654 R1: 0.299000 R5 0.539600 R10 0.640200 Mean 12.912800\n",
      "Epoch: 3 learningRate 0.000400 train loss 256.211792 Time: 30.974582\n",
      "Evaluating ... \n",
      "3/1000: mrr: 0.447145 R1: 0.326100 R5 0.557800 R10 0.664800 Mean 11.691200\n",
      "Epoch: 4 learningRate 0.000400 train loss 251.614697 Time: 30.963643\n",
      "Evaluating ... \n",
      "4/1000: mrr: 0.457496 R1: 0.329200 R5 0.579600 R10 0.690500 Mean 10.786000\n"
     ]
    }
   ],
   "source": [
    "lr = 0.0004\n",
    "beta1 = 0.8\n",
    "niter = 5\n",
    "neg_batch_sample = 30 \n",
    "log_interval = 50\n",
    "save_iter = 10000000\n",
    "save_path = '~/notebooks/saved_checkpoints'\n",
    "optimizer = optim.Adam([{'params': netW.parameters()},\n",
    "                        {'params': netE.parameters()},\n",
    "                        {'params': netD.parameters()}], lr=lr, betas=(beta1, 0.999))\n",
    "\n",
    "\n",
    "history = []\n",
    "\n",
    "for epoch in range(niter):\n",
    "    t = time.time()\n",
    "    train_loss = train(epoch)\n",
    "    print ('Epoch: %d learningRate %4f train loss %4f Time: %3f' % (epoch, lr, train_loss, time.time()-t))\n",
    "    train_his = {'loss': train_loss}\n",
    "\n",
    "    print('Evaluating ... ')\n",
    "    rank_all = val()\n",
    "    R1 = np.sum(np.array(rank_all)==1) / float(len(rank_all))\n",
    "    R5 =  np.sum(np.array(rank_all)<=5) / float(len(rank_all))\n",
    "    R10 = np.sum(np.array(rank_all)<=10) / float(len(rank_all))\n",
    "    ave = np.sum(np.array(rank_all)) / float(len(rank_all))\n",
    "    mrr = np.sum(1/(np.array(rank_all, dtype='float'))) / float(len(rank_all))\n",
    "    print ('%d/%d: mrr: %f R1: %f R5 %f R10 %f Mean %f' %(epoch, len(dataloader_val), mrr, R1, R5, R10, ave))\n",
    "    val_his = {'R1': R1, 'R5':R5, 'R10': R10, 'Mean':ave, 'mrr':mrr}\n",
    "    history.append({'epoch':epoch, 'train': train_his, 'val': val_his})\n",
    "\n",
    "    # saving the model.\n",
    "#     if epoch % save_iter == 0:\n",
    "#         torch.save({'epoch': epoch,\n",
    "#                     'netW': netW.state_dict(),\n",
    "#                     'netD': netD.state_dict(),\n",
    "#                     'netE': netE.state_dict()},\n",
    "#                     '%s/epoch_%d.pth' % (save_path, epoch))\n",
    "\n",
    "#         json.dump(history, open('%s/log.json' %(save_path), 'w'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
